{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from credentials import mysql_username, mysql_password\n",
    "\n",
    "# create the SparkSession\n",
    "spark = SparkSession.builder.appName('branch-pyspark').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+------------+------------+-------------------+----------+--------------------+\n",
      "|      BRANCH_CITY|BRANCH_CODE| BRANCH_NAME|BRANCH_PHONE|BRANCH_STATE|      BRANCH_STREET|BRANCH_ZIP|        LAST_UPDATED|\n",
      "+-----------------+-----------+------------+------------+------------+-------------------+----------+--------------------+\n",
      "|        Lakeville|          1|Example Bank|  1234565276|          MN|       Bridle Court|     55044|2018-04-18T16:51:...|\n",
      "|          Huntley|          2|Example Bank|  1234618993|          IL|  Washington Street|     60142|2018-04-18T16:51:...|\n",
      "|SouthRichmondHill|          3|Example Bank|  1234985926|          NY|      Warren Street|     11419|2018-04-18T16:51:...|\n",
      "|       Middleburg|          4|Example Bank|  1234663064|          FL|   Cleveland Street|     32068|2018-04-18T16:51:...|\n",
      "|    KingOfPrussia|          5|Example Bank|  1234849701|          PA|        14th Street|     19406|2018-04-18T16:51:...|\n",
      "|         Paterson|          7|Example Bank|  1234144890|          NJ|   Jefferson Street|      7501|2018-04-18T16:51:...|\n",
      "|        Pittsford|          8|Example Bank|  1234678272|          NY|           B Street|     14534|2018-04-18T16:51:...|\n",
      "|     Wethersfield|          9|Example Bank|  1234675219|          CT|    Jefferson Court|      6109|2018-04-18T16:51:...|\n",
      "|     NorthOlmsted|         10|Example Bank|  1234145047|          OH|     Cambridge Road|     44070|2018-04-18T16:51:...|\n",
      "|     Hillsborough|         11|Example Bank|  1234366354|          NJ|    3rd Street West|      8844|2018-04-18T16:51:...|\n",
      "|   MadisonHeights|         12|Example Bank|  1234867175|          MI|          Mill Road|     48071|2018-04-18T16:51:...|\n",
      "|           Oviedo|         14|Example Bank|  1234938460|          FL|  Washington Street|     32765|2018-04-18T16:51:...|\n",
      "|    Mechanicsburg|         15|Example Bank|  1234462043|          PA|    Chestnut Street|     17050|2018-04-18T16:51:...|\n",
      "|        Plainview|         16|Example Bank|  1234857525|          NY|       Monroe Drive|     11803|2018-04-18T16:51:...|\n",
      "|          Paducah|         17|Example Bank|  1234546360|          KY|    Railroad Street|     42001|2018-04-18T16:51:...|\n",
      "|     Harleysville|         18|Example Bank|  1234824455|          PA|Church Street South|     19438|2018-04-18T16:51:...|\n",
      "|     SilverSpring|         19|Example Bank|  1234484380|          MD|        King Street|     20901|2018-04-18T16:51:...|\n",
      "|       Burnsville|         20|Example Bank|  1234840138|          MN|   Canterbury Drive|     55337|2018-04-18T16:51:...|\n",
      "|           Tacoma|         21|Example Bank|  1234362433|          WA|         2nd Avenue|     98444|2018-04-18T16:51:...|\n",
      "|         Carlisle|         22|Example Bank|  1234922492|          PA| Front Street South|     17013|2018-04-18T16:51:...|\n",
      "+-----------------+-----------+------------+------------+------------+-------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load / Read data from json file:\n",
    "df_branch = spark.read.json(\"json_source_data/cdw_sapp_branch.json\")\n",
    "df_branch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BRANCH_CITY: string (nullable = true)\n",
      " |-- BRANCH_CODE: long (nullable = true)\n",
      " |-- BRANCH_NAME: string (nullable = true)\n",
      " |-- BRANCH_PHONE: string (nullable = true)\n",
      " |-- BRANCH_STATE: string (nullable = true)\n",
      " |-- BRANCH_STREET: string (nullable = true)\n",
      " |-- BRANCH_ZIP: long (nullable = true)\n",
      " |-- LAST_UPDATED: string (nullable = true)\n",
      "\n",
      "+-------+-----------+-----------------+------------+--------------------+------------+-------------+------------------+--------------------+\n",
      "|summary|BRANCH_CITY|      BRANCH_CODE| BRANCH_NAME|        BRANCH_PHONE|BRANCH_STATE|BRANCH_STREET|        BRANCH_ZIP|        LAST_UPDATED|\n",
      "+-------+-----------+-----------------+------------+--------------------+------------+-------------+------------------+--------------------+\n",
      "|  count|        115|              115|         115|                 115|         115|          115|               115|                 115|\n",
      "|   mean|       null|76.67826086956522|        null|1.2345499259478261E9|        null|         null|  38975.2347826087|                null|\n",
      "| stddev|       null|52.94113709535237|        null|  258751.74757815443|        null|         null|23938.156819564818|                null|\n",
      "|    min|    Acworth|                1|Example Bank|          1234105725|          AL|  11th Street|              2155|2018-04-18T16:51:...|\n",
      "|    max|   YubaCity|              197|Example Bank|          1234985926|          WI|  York Street|             98908|2018-04-18T16:51:...|\n",
      "+-------+-----------+-----------------+------------+--------------------+------------+-------------+------------------+--------------------+\n",
      "\n",
      "+-----------------+-----------+------------+------------+------------+-------------------+----------+--------------------+\n",
      "|      BRANCH_CITY|BRANCH_CODE| BRANCH_NAME|BRANCH_PHONE|BRANCH_STATE|      BRANCH_STREET|BRANCH_ZIP|        LAST_UPDATED|\n",
      "+-----------------+-----------+------------+------------+------------+-------------------+----------+--------------------+\n",
      "|        Lakeville|          1|Example Bank|  1234565276|          MN|       Bridle Court|     55044|2018-04-18T16:51:...|\n",
      "|          Huntley|          2|Example Bank|  1234618993|          IL|  Washington Street|     60142|2018-04-18T16:51:...|\n",
      "|SouthRichmondHill|          3|Example Bank|  1234985926|          NY|      Warren Street|     11419|2018-04-18T16:51:...|\n",
      "|       Middleburg|          4|Example Bank|  1234663064|          FL|   Cleveland Street|     32068|2018-04-18T16:51:...|\n",
      "|    KingOfPrussia|          5|Example Bank|  1234849701|          PA|        14th Street|     19406|2018-04-18T16:51:...|\n",
      "|         Paterson|          7|Example Bank|  1234144890|          NJ|   Jefferson Street|      7501|2018-04-18T16:51:...|\n",
      "|        Pittsford|          8|Example Bank|  1234678272|          NY|           B Street|     14534|2018-04-18T16:51:...|\n",
      "|     Wethersfield|          9|Example Bank|  1234675219|          CT|    Jefferson Court|      6109|2018-04-18T16:51:...|\n",
      "|     NorthOlmsted|         10|Example Bank|  1234145047|          OH|     Cambridge Road|     44070|2018-04-18T16:51:...|\n",
      "|     Hillsborough|         11|Example Bank|  1234366354|          NJ|    3rd Street West|      8844|2018-04-18T16:51:...|\n",
      "|   MadisonHeights|         12|Example Bank|  1234867175|          MI|          Mill Road|     48071|2018-04-18T16:51:...|\n",
      "|           Oviedo|         14|Example Bank|  1234938460|          FL|  Washington Street|     32765|2018-04-18T16:51:...|\n",
      "|    Mechanicsburg|         15|Example Bank|  1234462043|          PA|    Chestnut Street|     17050|2018-04-18T16:51:...|\n",
      "|        Plainview|         16|Example Bank|  1234857525|          NY|       Monroe Drive|     11803|2018-04-18T16:51:...|\n",
      "|          Paducah|         17|Example Bank|  1234546360|          KY|    Railroad Street|     42001|2018-04-18T16:51:...|\n",
      "|     Harleysville|         18|Example Bank|  1234824455|          PA|Church Street South|     19438|2018-04-18T16:51:...|\n",
      "|     SilverSpring|         19|Example Bank|  1234484380|          MD|        King Street|     20901|2018-04-18T16:51:...|\n",
      "|       Burnsville|         20|Example Bank|  1234840138|          MN|   Canterbury Drive|     55337|2018-04-18T16:51:...|\n",
      "|           Tacoma|         21|Example Bank|  1234362433|          WA|         2nd Avenue|     98444|2018-04-18T16:51:...|\n",
      "|         Carlisle|         22|Example Bank|  1234922492|          PA| Front Street South|     17013|2018-04-18T16:51:...|\n",
      "+-----------------+-----------+------------+------------+------------+-------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_branch.columns\n",
    "df_branch.printSchema()\n",
    "df_branch.describe().show()\n",
    "df_branch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|count(DISTINCT BRANCH_PHONE)|\n",
      "+----------------------------+\n",
      "|                         115|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_branch.select(F.countDistinct(\"BRANCH_PHONE\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+\n",
      "| BRANCH_CITY|BRANCH_STATE|BRANCH_ZIP|\n",
      "+------------+------------+----------+\n",
      "|    Paterson|          NJ|      7501|\n",
      "|Wethersfield|          CT|      6109|\n",
      "|Hillsborough|          NJ|      8844|\n",
      "|     Medford|          MA|      2155|\n",
      "|    Rockaway|          NJ|      7866|\n",
      "|  LongBranch|          NJ|      7740|\n",
      "|   Irvington|          NJ|      7111|\n",
      "|    NewHaven|          CT|      6511|\n",
      "|      Quincy|          MA|      2169|\n",
      "+------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find zip codes with less than 5 values\n",
    "df_branch.filter(F.length(df_branch[\"BRANCH_ZIP\"]) < 5)\\\n",
    "         .select('BRANCH_CITY', 'BRANCH_STATE', 'BRANCH_ZIP')\\\n",
    "         .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 0 infront of zip codes with 4 values\n",
    "df_branch = df_branch.withColumn('BRANCH_ZIP',\\\n",
    "                    F.when((F.length(df_branch['BRANCH_ZIP']) == 4) &\n",
    "                        df_branch['BRANCH_STATE'].isin([\"NJ\", \"CT\", \"MA\"]),\n",
    "                    F.format_string(\"0%s\",df_branch['BRANCH_ZIP']))\\\n",
    "                    .otherwise(df_branch[\"BRANCH_ZIP\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|BRANCH_ZIP|\n",
      "+----------+\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify there are no more zip codes with less than 5 values\n",
    "df_branch.select('BRANCH_ZIP')\\\n",
    "    .where(F.length(df_branch[\"BRANCH_ZIP\"]) < 4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_branch = df_branch.withColumn('BRANCH_PHONE',\n",
    "            F.format_string(\"(%s)%s-%s\", \n",
    "                            F.substring(df_branch['BRANCH_PHONE'], 1, 3), \n",
    "                            F.substring(df_branch['BRANCH_PHONE'], 4, 3), \n",
    "                            F.substring(df_branch['BRANCH_PHONE'], 7, 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_branch = df_branch.withColumn(\n",
    "    'BRANCH_CITY',\n",
    "    F.regexp_replace(df_branch['BRANCH_CITY'], \"(?<=.)([A-Z])\", ' $1')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------------+-------------------+------------+----------+-------------+--------------------+\n",
      "|BRANCH_CODE| BRANCH_NAME|      BRANCH_STREET|        BRANCH_CITY|BRANCH_STATE|BRANCH_ZIP| BRANCH_PHONE|        LAST_UPDATED|\n",
      "+-----------+------------+-------------------+-------------------+------------+----------+-------------+--------------------+\n",
      "|          1|Example Bank|       Bridle Court|          Lakeville|          MN|     55044|(123)456-5276|2018-04-18T16:51:...|\n",
      "|          2|Example Bank|  Washington Street|            Huntley|          IL|     60142|(123)461-8993|2018-04-18T16:51:...|\n",
      "|          3|Example Bank|      Warren Street|South Richmond Hill|          NY|     11419|(123)498-5926|2018-04-18T16:51:...|\n",
      "|          4|Example Bank|   Cleveland Street|         Middleburg|          FL|     32068|(123)466-3064|2018-04-18T16:51:...|\n",
      "|          5|Example Bank|        14th Street|    King Of Prussia|          PA|     19406|(123)484-9701|2018-04-18T16:51:...|\n",
      "|          7|Example Bank|   Jefferson Street|           Paterson|          NJ|     07501|(123)414-4890|2018-04-18T16:51:...|\n",
      "|          8|Example Bank|           B Street|          Pittsford|          NY|     14534|(123)467-8272|2018-04-18T16:51:...|\n",
      "|          9|Example Bank|    Jefferson Court|       Wethersfield|          CT|     06109|(123)467-5219|2018-04-18T16:51:...|\n",
      "|         10|Example Bank|     Cambridge Road|      North Olmsted|          OH|     44070|(123)414-5047|2018-04-18T16:51:...|\n",
      "|         11|Example Bank|    3rd Street West|       Hillsborough|          NJ|     08844|(123)436-6354|2018-04-18T16:51:...|\n",
      "|         12|Example Bank|          Mill Road|    Madison Heights|          MI|     48071|(123)486-7175|2018-04-18T16:51:...|\n",
      "|         14|Example Bank|  Washington Street|             Oviedo|          FL|     32765|(123)493-8460|2018-04-18T16:51:...|\n",
      "|         15|Example Bank|    Chestnut Street|      Mechanicsburg|          PA|     17050|(123)446-2043|2018-04-18T16:51:...|\n",
      "|         16|Example Bank|       Monroe Drive|          Plainview|          NY|     11803|(123)485-7525|2018-04-18T16:51:...|\n",
      "|         17|Example Bank|    Railroad Street|            Paducah|          KY|     42001|(123)454-6360|2018-04-18T16:51:...|\n",
      "|         18|Example Bank|Church Street South|       Harleysville|          PA|     19438|(123)482-4455|2018-04-18T16:51:...|\n",
      "|         19|Example Bank|        King Street|      Silver Spring|          MD|     20901|(123)448-4380|2018-04-18T16:51:...|\n",
      "|         20|Example Bank|   Canterbury Drive|         Burnsville|          MN|     55337|(123)484-0138|2018-04-18T16:51:...|\n",
      "|         21|Example Bank|         2nd Avenue|             Tacoma|          WA|     98444|(123)436-2433|2018-04-18T16:51:...|\n",
      "|         22|Example Bank| Front Street South|           Carlisle|          PA|     17013|(123)492-2492|2018-04-18T16:51:...|\n",
      "+-----------+------------+-------------------+-------------------+------------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_branch.columns\n",
    "df_branch = df_branch.select(\"BRANCH_CODE\", \"BRANCH_NAME\", \"BRANCH_STREET\", \"BRANCH_CITY\", \"BRANCH_STATE\", \"BRANCH_ZIP\", \"BRANCH_PHONE\", \"LAST_UPDATED\")\n",
    "df_branch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BRANCH_CODE: long (nullable = true)\n",
      " |-- BRANCH_NAME: string (nullable = true)\n",
      " |-- BRANCH_STREET: string (nullable = true)\n",
      " |-- BRANCH_CITY: string (nullable = true)\n",
      " |-- BRANCH_STATE: string (nullable = true)\n",
      " |-- BRANCH_ZIP: string (nullable = true)\n",
      " |-- BRANCH_PHONE: string (nullable = false)\n",
      " |-- LAST_UPDATED: string (nullable = true)\n",
      "\n",
      "+-------+-----------------+------------+-------------+-----------+------------+------------------+-------------+--------------------+\n",
      "|summary|      BRANCH_CODE| BRANCH_NAME|BRANCH_STREET|BRANCH_CITY|BRANCH_STATE|        BRANCH_ZIP| BRANCH_PHONE|        LAST_UPDATED|\n",
      "+-------+-----------------+------------+-------------+-----------+------------+------------------+-------------+--------------------+\n",
      "|  count|              115|         115|          115|        115|         115|               115|          115|                 115|\n",
      "|   mean|76.67826086956522|        null|         null|       null|        null|  38975.2347826087|         null|                null|\n",
      "| stddev|52.94113709535237|        null|         null|       null|        null|23938.156819564818|         null|                null|\n",
      "|    min|                1|Example Bank|  11th Street|    Acworth|          AL|             02155|(123)410-5725|2018-04-18T16:51:...|\n",
      "|    max|              197|Example Bank|  York Street|  Yuba City|          WI|             98908|(123)498-5926|2018-04-18T16:51:...|\n",
      "+-------+-----------------+------------+-------------+-----------+------------+------------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_branch.printSchema()\n",
    "df_branch.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o111.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 11) (LAPTOP-IQMPCJRO executor driver): java.sql.BatchUpdateException: Data truncation: Data too long for column 'BRANCH_PHONE' at row 1\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:794)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Data too long for column 'BRANCH_PHONE' at row 1\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:104)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1092)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.sql.BatchUpdateException: Data truncation: Data too long for column 'BRANCH_PHONE' at row 1\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:794)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Data too long for column 'BRANCH_PHONE' at row 1\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:104)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1092)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fube9\\Documents\\GitHub\\Data-Engineer-Capstone-Project-\\branch_table_etl.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fube9/Documents/GitHub/Data-Engineer-Capstone-Project-/branch_table_etl.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_branch\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mjdbc\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fube9/Documents/GitHub/Data-Engineer-Capstone-Project-/branch_table_etl.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   \u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39mappend\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fube9/Documents/GitHub/Data-Engineer-Capstone-Project-/branch_table_etl.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mjdbc:mysql://localhost:3306/creditcard_capstone\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fube9/Documents/GitHub/Data-Engineer-Capstone-Project-/branch_table_etl.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mdbtable\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcreditcard_capstone.cdw_sapp_branch\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fube9/Documents/GitHub/Data-Engineer-Capstone-Project-/branch_table_etl.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, mysql_username) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fube9/Documents/GitHub/Data-Engineer-Capstone-Project-/branch_table_etl.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mpassword\u001b[39;49m\u001b[39m\"\u001b[39;49m, mysql_password) \\\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fube9/Documents/GitHub/Data-Engineer-Capstone-Project-/branch_table_etl.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   \u001b[39m.\u001b[39;49msave()\n",
      "File \u001b[1;32mc:\\Users\\fube9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1396\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[0;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1396\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[1;32mc:\\Users\\fube9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\fube9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\fube9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o111.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 11) (LAPTOP-IQMPCJRO executor driver): java.sql.BatchUpdateException: Data truncation: Data too long for column 'BRANCH_PHONE' at row 1\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:794)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Data too long for column 'BRANCH_PHONE' at row 1\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:104)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1092)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.sql.BatchUpdateException: Data truncation: Data too long for column 'BRANCH_PHONE' at row 1\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:794)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:740)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Data too long for column 'BRANCH_PHONE' at row 1\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:104)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1092)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "df_branch.write.format(\"jdbc\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"url\", \"jdbc:mysql://localhost:3306/creditcard_capstone\") \\\n",
    "  .option(\"dbtable\", \"creditcard_capstone.cdw_sapp_branch\") \\\n",
    "  .option(\"user\", mysql_username) \\\n",
    "  .option(\"password\", mysql_password) \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
